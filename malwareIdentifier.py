#!/usr/bin/env python
__author__ = "Rishav Rajendra"

import pandas as pd
import numpy as np
from sklearn.feature_selection import mutual_info_classif
from sklearn.decomposition import PCA
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
from xgboost import XGBClassifier
from mlxtend.classifier import StackingCVClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import ElasticNet
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import BernoulliNB
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.tree import DecisionTreeClassifier

def main():
	print('Loading Feature Set')
	dataset = pd.read_csv("vector_1000.svm", header = None)
	print('Feature set loaded')

	# X = dataset.iloc[:, 1:3] + dataset.iloc[:, 5:10] + dataset.iloc[:, 11:13]	#Not using column 13 as it's NaN
	X = dataset.iloc[:, 1:13]

	Y = dataset.iloc[:, 0]			#The first column
	print(Y)

	print('X and Y created')

	#Features dropped due to no information gain
	X = X.drop([3, 4, 5, 10], axis=1)

	#Calculate information gain
	res = mutual_info_classif(X, Y, discrete_features=False)
	print("\nInformation Gain")
	print(res)

	pca = PCA(n_components = 8)
	print("\n\nPCA")
	print(pca.fit(X.values).components_)
	print("\n\nFeature importance")
	print(ExtraTreesClassifier().fit(X.values, Y.values).feature_importances_)

	clf_AdaBoost = AdaBoostClassifier()
	clf_XGBoost = XGBClassifier(n_jobs=-1)
	clf_ExtraTrees = ExtraTreesClassifier(n_jobs=-1)
	clf_KNeighbors = KNeighborsClassifier(n_jobs=-1)
	clf_DecisionTree = DecisionTreeClassifier()
	clf_GaussianNB = GaussianNB()
	clf_Bernoulli = BernoulliNB()

	#Performing GridSearch for the parameters for XGBClassifier
	XGBCparams = {
		'max_depth':range(3, 10, 1),
		'min_child_weight':range(1, 12, 1),
		'gamma':[i/10.0 for i in range(0,5)],
	}

	XGBGrid = GridSearchCV(estimator = clf_XGBoost, 
							param_grid = XGBCparams,
							iid = False,
							cv = 10)

	XGB_Grid_Result = XGBGrid.fit(np.array(X), np.array(Y))

	#Adding the best parameters of GridSearch for Stacking
	clf_XGBoost = XGBClassifier(**XGB_Grid_Result.best_params_, n_jobs=-1)

	#Performing GridSearch for the parameters for ExtraParams
	ExtraParams = {
		'n_estimators': range(50,126,25),
        'max_features': range(1,8),
        'min_samples_leaf': range(20,50,5),
        'min_samples_split': range(15,36,5),
	}

	ExtraGrid = GridSearchCV(estimator = clf_ExtraTrees,
							param_grid = ExtraParams,
							cv = 10,
							iid = False)

	Extra_Grid_Result = ExtraGrid.fit(np.array(X), np.array(Y))

	#Adding the best parameters of GridSearch for Stacking
	clf_ExtraTrees = ExtraTreesClassifier(**Extra_Grid_Result.best_params_, n_jobs=-1)

	#Performing GridSearch for the best parameters for AdaBoost
	AdaBoostParams = {
		'n_estimators': [50, 100],
		'learning_rate' : [0.01,0.05,0.1,0.3,1],
		'loss' : ['linear', 'square', 'exponential']
	}

	AdaBoostGrid = GridSearchCV(estimator = clf_AdaBoost,
								param_grid = AdaBoostParams,
								cv=10,
								iid=False)

	AdaBoost_Grid_Result = AdaBoostGrid.fit(np.array(X), np.array(Y))

	#Adding the best parameters of GridSearch for Stacking
	clf_AdaBoost = AdaBoostClassifier(**AdaBoost_Grid_Result.best_params_)

	#Performing GridSearch for the best parameters for KNN
	knn_range = list(range(1, 31))
	KNN_params = dict(n_neighbors=knn_range)

	KNN_grid = GridSearchCV(clf_KNeighbors, 
							KNN_params, 
							cv=10, 
							scoring='accuracy')

	KNN_grid_result = KNN_grid.fit(np.array(X), np.array(Y))

	#Adding the best parameters of GridSearch for Stacking
	clf_KNeighbors = KNeighborsClassifier(**KNN_grid_result.best_params_)

	#Performing GridSearch for the best parameters for LR
	lr_grid = GridSearchCV(cv=None,
             				estimator=LogisticRegression(C=1.0, intercept_scaling=1,   
               				dual=False, fit_intercept=True, penalty='l2', tol=0.0001),
             				param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]})

	lr_grid_result = lr_grid.fit(np.array(X), np.array(Y))

	#Adding the best parameters of GridSearch for Stacking
	clf_LogisticRegression = LogisticRegression(**KNN_grid_result.best_params_)

	#Stack the best classifiers
	clf_StackingCVClassifer = StackingCVClassifier(classifiers=[clf_AdaBoost, clf_XGBoost, clf_ExtraTrees, clf_KNeighbors], meta_classifier=clf_LogisticRegression, use_probas=True)

	for clf, label in zip([clf_AdaBoost, clf_XGBoost, clf_ExtraTrees, clf_KNeighbors, clf_DecisionTree, clf_LogisticRegression, clf_GaussianNB, clf_Bernoulli, clf_StackingCVClassifer], 
		['AdaBoostClassifier', 'XGBClassifier', 'ExtraTreesClassifier', 'KNeighborsClassifier', 'DecisionTreeClassifier', 'LogisticRegression', 'GaussianNB', 'BernoulliNB', 'StackingCVClassifer']):
		scores = cross_val_score(clf, np.array(X), np.array(Y), cv = 10, scoring='accuracy')
		print('Accuracy: %0.2f (+/- %0.2f) [%s]'
			%(scores.mean(), scores.std(), label))

if __name__ == '__main__':
	main()