import pandas as pd
import numpy as np
from xgboost import XGBClassifier
from sklearn.cross_validation import train_test_split
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import cross_val_predict
from sklearn.model_selection import GridSearchCV
from mlxtend.classifier import StackingCVClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
import multiprocessing

if __name__ == "__main__":

    outputFile = open('resultsForSampleData.txt', 'a+')

    print 'Loading Features Set'
    dataset = pd.read_csv("vector_100files.svm", header = None)
    print 'Feature Set Loaded'

    #Splitting data into X and Y
    X = dataset.iloc[:, 1:] #all columns from 1 to the end
    Y = dataset.iloc[:, 0] #the first column
    print 'X and Y created'

    #XGBClassifier = XGBClassifier(**XGBParams)
    params = {
            'xgbclassifier__max_depth':[2,3,4],
            'xgbclassifier__min_child_weight':[2,3,4]
            }

    #Load classifiers
    clf1 = XGBClassifier(learning_rate=0.1, n_estimators=140, max_depth=5,
                                    min_child_weight=3, gamma=0, subsample=0.8, colsample_bytree=0.8,
                                    objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27)
    #clf_mlp_classifier = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(50, 20), random_state=1)
    clf2 = SVC(C=55, gamma=0.0001, random_state=12, probability=True)
    lr = LogisticRegression()
    sclf = StackingCVClassifier(classifiers=[clf1, clf2], meta_classifier=lr, use_probas=True)

    grid = GridSearchCV(estimator = sclf,
                            param_grid = params,
                            cv=5,
                            scoring='roc_auc',
                            n_jobs=4,
                            iid=False)

    grid.fit(np.array(X),np.array(Y))

    print(grid.grid_scores_, grid.best_params_, grid.best_score_)

    #Split features into test and train
    # kfold = KFold(n_splits=10, random_state=42)
    #
    # #Training classifiers
    # #NOTE: DO NOT USE n_jobs if setting XGBParams nthread option. The program will freeze.
    # print '-'*32 + 'Started Training' + '-'*32
    # trained_data = cross_val_score(XGBClassifier, np.array(X), np.array(Y), cv=kfold)
    # print 'Done Training'
    # print '-'*31 + 'Starting Prediction' + '-'*30
    # y_predict = cross_val_predict(XGBClassifier, np.array(X), np.array(Y), cv=kfold)
    # print '-'*32 + 'Done predicting' + '-'*32
    #
    # # predict_proba = cross_val_predict(XGBClassifier, np.array(X), np.array(Y), cv=kfold, n_jobs = 1, method='predict_proba')
    # # np.set_printoptions(threshold=np.nan)
    # # print('Predicted probabilities for train: ') + np.matrix(predict_proba)
    #
    # confusion = confusion_matrix(Y, y_predict)
    #
    # print '-'*32 + 'Answers: ' + '-'*32
    # outputFile.write('Accuracy: %f \n'%(y_predict.mean()*100))
    #
    # outputFile.write('Confustion Matrix: %r \n'%confusion)
