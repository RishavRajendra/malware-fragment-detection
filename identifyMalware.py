import pandas as pd
import numpy as np
from xgboost import XGBClassifier
from sklearn.cross_validation import train_test_split
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import cross_val_predict
from sklearn.model_selection import GridSearchCV
from mlxtend.classifier import StackingCVClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score

if __name__ == "__main__":

    #Output file to write the results
    outputFile = open('resultsForSampleData.txt', 'a+')

    print 'Loading Features Set'
    dataset = pd.read_csv("vector_100files.svm", header = None)
    print 'Feature Set Loaded'

    #Splitting data into X and Y
    X = dataset.iloc[:, 1:] #all columns from 1 to the end
    Y = dataset.iloc[:, 0] #the first column
    print 'X and Y created'

    #Parameters for StackingCVClassifier
    params = {
            'xgbclassifier__max_depth':range(3, 10, 1),
            'xgbclassifier__min_child_weight':range(1, 12, 1),
            'xgbclassifier__gamma':[i/10.0 for i in range(0,5)],
            'xgbclassifier__subsample':[i/10.0 for i in range(6,10)],
            'xgbclassifier__colsample_bytree':[i/10.0 for i in range(6,10)],
            'xgbclassifier__reg_alpha':[1e-5, 1e-2, 0.1, 1, 100],
            'svc__C':[0.001, 0.01, 0.1, 1, 10],
            'svc__gamma':[0.001, 0.01, 0.1, 1]
            }

    #Load classifiers
    clf1 = XGBClassifier(learning_rate =0.01, n_estimators=4000, max_depth=4,
            min_child_weight=6, gamma=0.1, subsample=0.8, colsample_bytree=0.8,
            objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27)
    #clf_mlp_classifier = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(50, 20), random_state=1)
    clf2 = SVC(C=55, gamma=0.0001, random_state=12, probability=True)
    lr = LogisticRegression()
    sclf = StackingCVClassifier(classifiers=[clf1, clf2], meta_classifier=lr, use_probas=True)

    #GridSearchCV to tune the classifiers
    grid = GridSearchCV(estimator = sclf,
                            param_grid = params,
                            n_jobs=-1,
                            iid=False)

    grid.fit(X, Y)
    print(grid.grid_scores_, grid.best_params_, grid.best_score_)

    #Split features into test and train
    kfold = KFold(n_splits=10, random_state=42)

    #Training classifiers
    #NOTE: DO NOT USE n_jobs if setting XGBParams nthread option. The program will freeze.
    print '-'*32 + 'Started Training' + '-'*32
    trained_data = cross_val_score(grid, np.array(X), np.array(Y), cv=kfold)
    print 'Done Training'
    print '-'*31 + 'Starting Prediction' + '-'*30
    y_predict = cross_val_predict(grid, np.array(X), np.array(Y), cv=kfold)
    print '-'*32 + 'Done predicting' + '-'*32

    #The performance of the a classification model
    confusion = confusion_matrix(Y, y_predict)

    print '-'*32 + 'Answers: ' + '-'*32
    #Write the Accuracy of the classification model to the output file
    outputFile.write('Accuracy: %f \n'%(y_predict.mean()*100))
    #Write the Confusion Matrix of the classification model to the output file
    outputFile.write('Confustion Matrix: %r \n'%confusion)
