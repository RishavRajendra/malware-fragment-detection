import pandas as pd
import numpy as np
from xgboost import XGBClassifier
from sklearn import model_selection
from sklearn.neural_network import MLPClassifier
from sklearn.cross_validation import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import cross_val_predict
from sklearn.model_selection import GridSearchCV
from mlxtend.classifier import StackingCVClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score

if __name__ == "__main__":

    #Output file to write the results
    outputFile = open('resultsForSampleData.txt', 'a+')

    print 'Loading Features Set'
    dataset = pd.read_csv("vector_1000FilesEach.svm", header = None)
    print 'Feature Set Loaded'

    #Splitting data into X and Y
    X = dataset.iloc[:, 1:] #all columns from 1 to the end
    Y = dataset.iloc[:, 0] #the first column
    print 'X and Y created'

    #Parameters for StackingCVClassifier
    params = {
     #       'xgbclassifier__max_depth':range(3, 10, 1),
      #      'xgbclassifier__min_child_weight':range(1, 12, 1),
       #     'xgbclassifier__gamma':[i/10.0 for i in range(0,5)],
        #    'xgbclassifier__subsample':[i/10.0 for i in range(6,10)],
         #   'xgbclassifier__colsample_bytree':[i/10.0 for i in range(6,10)],
          #  'xgbclassifier__reg_alpha':[1e-5, 1e-2, 0.1, 1, 100],
           # 'svc__C':[0.001, 0.01, 0.1, 1, 10],
            #'svc__gamma':[0.001, 0.01, 0.1, 1]
            }

    #Load classifiers

    #Gradient Boosting Classifer
    clf_XGB = XGBClassifier(learning_rate =0.05, n_estimators=100, max_depth=4,
            min_child_weight=6, gamma=0, subsample=0.8, colsample_bytree=0.6,
            objective= 'binary:logistic', scale_pos_weight=1,seed=27)
    clf_KNN = KNeighborsClassifier(n_neighbors=1)
    clf_mlp_classifier = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(50, 20), random_state=1)
    clf_gaussian = GaussianNB()

    lr = LogisticRegression()

    #StackingCVClassifier to stack multiple classifiers
    sclf = StackingCVClassifier(classifiers=[clf_KNN, clf_XGB, clf_mlp_classifier, clf_gaussian], meta_classifier=lr, use_probas=True)

    #GridSearchCV to tune the classifiers
    # grid = GridSearchCV(estimator = sclf,
    #                         param_grid = params,
    #                         n_jobs=-1,
    #                         iid=False)

    #Split features into test and train
    X_train, X_test, Y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

    #Training classifiers
    #NOTE: DO NOT USE n_jobs if setting XGBParams nthread option. The program will freeze.

    print '-'*32 + 'Answers: ' + '-'*32

    #The performance of the a classification model
    #confusion = confusion_matrix(Y, y_predict)
    #print(confusion)

    #Calculate the accuracy

    for clf, label in zip([clf_KNN, clf_XGB, clf_mlp_classifier, clf_gaussian, sclf],
                      ['KNN',
                      'XGB',
                       'MLP',
                       'Gaussian',
                       'StackingCVClassifier']):
                       scores = model_selection.cross_val_score(clf, np.array(X), np.array(Y), cv=10, scoring='accuracy')
                       print("Accuracy: %0.2f (+/- %0.2f) [%s]"
                       % (scores.mean(), scores.std(), label))
