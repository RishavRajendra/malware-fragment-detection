import pandas as pd
import numpy as np
from xgboost import XGBClassifier
from sklearn import model_selection
from sklearn.neural_network import MLPClassifier
from sklearn.cross_validation import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import cross_val_predict
from sklearn.model_selection import GridSearchCV
from mlxtend.classifier import StackingCVClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.decomposition import PCA

if __name__ == "__main__":

    #Output file to write the results
    outputFile = open('resultsForSampleData.txt', 'a+')

    print 'Loading Features Set'
    dataset = pd.read_csv("vectormanish.svm", header = None)
    print 'Feature Set Loaded'

    #Splitting data into X and Y
    X = dataset.iloc[:, 1:] #all columns from 1 to the end
    Y = dataset.iloc[:, 0] #the first column
    print 'X and Y created'

    #Parameters for StackingCVClassifier
    params = {
           'xgbclassifier__max_depth':range(3, 10, 1),
           'xgbclassifier__min_child_weight':range(1, 12, 1),
           'xgbclassifier__gamma':[i/10.0 for i in range(0,5)],
        #    'xgbclassifier__subsample':[i/10.0 for i in range(6,10)],
         #   'xgbclassifier__colsample_bytree':[i/10.0 for i in range(6,10)],
          #  'xgbclassifier__reg_alpha':[1e-5, 1e-2, 0.1, 1, 100],
           # 'svc__C':[0.001, 0.01, 0.1, 1, 10],
            #'svc__gamma':[0.001, 0.01, 0.1, 1]
            }

    #Load classifiers

    #Gradient Boosting Classifer
    pca = PCA(n_components=10)
    print dataset.shape
    print pca.fit(X.values).components_
    print ExtraTreesClassifier().fit(X.values, Y.values).feature_importances_
    clf_XGB = XGBClassifier(learning_rate =0.05, n_estimators=100, max_depth=4,
            min_child_weight=6, gamma=0, subsample=0.8, colsample_bytree=0.6,
            objective= 'binary:logistic', scale_pos_weight=1,seed=27)
    clf_KNN = KNeighborsClassifier(n_neighbors=1)
    clf_gaussian = GaussianNB()

    lr = LogisticRegression()

    #StackingCVClassifier to stack multiple classifiers
    sclf = StackingCVClassifier(classifiers=[clf_KNN, clf_gaussian, clf_XGB, RandomForestClassifier(), SVC(probability=True) ], meta_classifier=lr, use_probas=True)

    #GridSearchCV to tune the classifiers
    grid = GridSearchCV(estimator = sclf,
                            param_grid = params,
                            n_jobs=-1,
                            iid=False)

    grid.fit(X.values, Y.values)

    cv_keys = ('mean_test_score', 'std_test_score', 'params')

    for r, _ in enumerate(grid.cv_results_['mean_test_score']):
    	print("%0.3f +/- %0.2f %r"
          % (grid.cv_results_[cv_keys[0]][r],
             grid.cv_results_[cv_keys[1]][r] / 2.0,
             grid.cv_results_[cv_keys[2]][r]))

    print('Best parameters: %s' % grid.best_params_)
    print('Accuracy: %.2f' % grid.best_score_)

    #Split features into test and train
    # X_train, X_test, Y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

    #Training classifiers
    #NOTE: DO NOT USE n_jobs if setting XGBParams nthread option. The program will freeze.

    print '-'*32 + 'Answers: ' + '-'*32

    #The performance of the a classification model
    #confusion = confusion_matrix(Y, y_predict)
    #print(confusion)

    #Calculate the accuracy

    # for clf, label in zip([clf_KNN, clf_XGB, clf_gaussian, sclf],
    #                   ['KNN',
    #                   'XGB',
    #                    'Gaussian',
    #                    'StackingCVClassifier']):
    #                    scores = model_selection.cross_val_score(clf, np.array(X), np.array(Y), cv=10, scoring='accuracy', n_jobs=-1)
    #                    print("Accuracy: %0.2f (+/- %0.2f) [%s]"
    #                    % (scores.mean(), scores.std(), label))
